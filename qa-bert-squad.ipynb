{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This will delete the entire folder and everything inside it.\n!rm -r question-answer-bert-squad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:36:58.020104Z","iopub.execute_input":"2025-09-30T15:36:58.020339Z","iopub.status.idle":"2025-09-30T15:36:58.141352Z","shell.execute_reply.started":"2025-09-30T15:36:58.020322Z","shell.execute_reply":"2025-09-30T15:36:58.140481Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove 'question-answer-bert-squad': No such file or directory\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Configure Git and Clone Repository\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\n# Get GitHub token from Kaggle Secrets\nuser_secrets = UserSecretsClient()\ngithub_token = user_secrets.get_secret(\"github_token\")\n\n# Configure Git with your email and username\n!git config --global user.email \"soumyagrandhi145@gmail.com\"\n!git config --global user.name \"soumya\"\n\n# Clone the repository using the token\nrepo_url = f\"https://soumya-grandhi:{github_token}@github.com/soumya-grandhi/question-answer-bert-squad.git\"\n!git clone {repo_url}\n\n# Change directory into the cloned repo\nos.chdir(\"question-answer-bert-squad\")\nprint(\"Changed working directory to:\", os.getcwd())\n!ls -a","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:36:58.142660Z","iopub.execute_input":"2025-09-30T15:36:58.142965Z","iopub.status.idle":"2025-09-30T15:36:59.355418Z","shell.execute_reply.started":"2025-09-30T15:36:58.142926Z","shell.execute_reply":"2025-09-30T15:36:59.354489Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'question-answer-bert-squad'...\nremote: Enumerating objects: 13, done.\u001b[K\nremote: Counting objects: 100% (13/13), done.\u001b[K\nremote: Compressing objects: 100% (10/10), done.\u001b[K\nremote: Total 13 (delta 1), reused 13 (delta 1), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (13/13), 992.12 KiB | 10.12 MiB/s, done.\nResolving deltas: 100% (1/1), done.\nChanged working directory to: /kaggle/working/question-answer-bert-squad\n.  ..  .git  outputs  requirements.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Create requirements.txt and install packages using standard Python\nrequirements_content = \"\"\"\ntorch\ntransformers>=4.40\ndatasets\npandas\n\"\"\"\n\nwith open(\"requirements.txt\", \"w\") as f:\n    f.write(requirements_content.strip())\n    \nprint(\"Successfully created requirements.txt\")\n!cat requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:37:00.126271Z","iopub.execute_input":"2025-09-30T15:37:00.127094Z","iopub.status.idle":"2025-09-30T15:37:00.255030Z","shell.execute_reply.started":"2025-09-30T15:37:00.127047Z","shell.execute_reply":"2025-09-30T15:37:00.254135Z"}},"outputs":[{"name":"stdout","text":"Successfully created requirements.txt\ntorch\ntransformers>=4.40\ndatasets\npandas","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Install from requirements.txt\n!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:37:03.398331Z","iopub.execute_input":"2025-09-30T15:37:03.399056Z","iopub.status.idle":"2025-09-30T15:38:12.512170Z","shell.execute_reply.started":"2025-09-30T15:37:03.399025Z","shell.execute_reply":"2025-09-30T15:38:12.511257Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\nRequirement already satisfied: transformers>=4.40 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.6.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 1))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.40->-r requirements.txt (line 2)) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 3)) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 3)) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 3)) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 3)) (0.70.16)\nCollecting fsspec (from torch->-r requirements.txt (line 1))\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.40->-r requirements.txt (line 2)) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.40->-r requirements.txt (line 2)) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.40->-r requirements.txt (line 2)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.40->-r requirements.txt (line 2)) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.40->-r requirements.txt (line 2)) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 3)) (1.20.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.40->-r requirements.txt (line 2)) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Create project directories\nimport os\n\nproject_dirs = [\"notebooks\", \"src\", \"outputs\"]\nfor dir_name in project_dirs:\n    os.makedirs(dir_name, exist_ok=True)\n    \nprint(\"Created directories:\", [d for d in os.listdir() if os.path.isdir(d)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:12.513683Z","iopub.execute_input":"2025-09-30T15:38:12.513917Z","iopub.status.idle":"2025-09-30T15:38:12.519500Z","shell.execute_reply.started":"2025-09-30T15:38:12.513895Z","shell.execute_reply":"2025-09-30T15:38:12.518852Z"}},"outputs":[{"name":"stdout","text":"Created directories: ['notebooks', 'outputs', 'src', '.git']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 5: Stage, commit, and push to GitHub\n!git add .\n!git commit -m \"feat: initial project setup and requirements\"\n!git push origin main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:12.520329Z","iopub.execute_input":"2025-09-30T15:38:12.520632Z","iopub.status.idle":"2025-09-30T15:38:13.129267Z","shell.execute_reply.started":"2025-09-30T15:38:12.520605Z","shell.execute_reply":"2025-09-30T15:38:13.128540Z"}},"outputs":[{"name":"stdout","text":"On branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\nEverything up-to-date\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"2. DATA LOADING AND INITIAL EXPLORATION","metadata":{}},{"cell_type":"markdown","source":"2.1 Load the Squad Dataset","metadata":{}},{"cell_type":"code","source":"# Cell 1: Load SQuAD dataset\nimport os\nfrom datasets import load_dataset, Dataset\n\n# The datasets library automatically downloads and caches the data\nsquad_dataset = load_dataset(\"squad_v2\")\n\n# Print the dataset structure\nprint(squad_dataset)\n\n# Show a sample from the training split\nprint(\"\\nSample from training split:\")\nprint(squad_dataset[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:13.131176Z","iopub.execute_input":"2025-09-30T15:38:13.131415Z","iopub.status.idle":"2025-09-30T15:38:18.789028Z","shell.execute_reply.started":"2025-09-30T15:38:13.131393Z","shell.execute_reply":"2025-09-30T15:38:18.788403Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48277bc131b0422f8f48999e28a0588e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"squad_v2/train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58d24e969b224ad4a447da41f3a26b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"squad_v2/validation-00000-of-00001.parqu(…):   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7e841b491741a2a1848eeb9690e9e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee39bab864e4373bf0db56f735f1feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b37df4e59f49e79ae888f6f6de26a5"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 130319\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 11873\n    })\n})\n\nSample from training split:\n{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"2.2 Basic Data Exploration","metadata":{"execution":{"iopub.status.busy":"2025-09-19T15:25:22.993645Z","iopub.execute_input":"2025-09-19T15:25:22.994160Z","iopub.status.idle":"2025-09-19T15:25:22.999433Z","shell.execute_reply.started":"2025-09-19T15:25:22.994135Z","shell.execute_reply":"2025-09-19T15:25:22.998428Z"}}},{"cell_type":"code","source":"# Cell 2: Explore dataset statistics\ntrain_data = squad_dataset[\"train\"]\nval_data = squad_dataset[\"validation\"]\n\n# Number of samples\nprint(f\"Number of training examples: {len(train_data)}\")\nprint(f\"Number of validation examples: {len(val_data)}\")\n\n# Get the distribution of answer lengths\ntrain_data_df = train_data.to_pandas()\nanswer_lengths = train_data_df['answers'].apply(lambda x: len(x['text'][0]) if x['text'] else 0)\nprint(\"\\nAnswer length statistics:\")\nprint(answer_lengths.describe())\n\n# Check for unanswerable questions in SQuAD v2\nunanswerable_questions = sum(1 for answers in train_data['answers'] if len(answers['text']) == 0)\nprint(f\"\\nNumber of unanswerable questions in training data: {unanswerable_questions}\")\nprint(f\"Percentage of unanswerable questions: {unanswerable_questions / len(train_data) * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:18.789686Z","iopub.execute_input":"2025-09-30T15:38:18.790083Z","iopub.status.idle":"2025-09-30T15:38:20.911083Z","shell.execute_reply.started":"2025-09-30T15:38:18.790061Z","shell.execute_reply":"2025-09-30T15:38:20.910293Z"}},"outputs":[{"name":"stdout","text":"Number of training examples: 130319\nNumber of validation examples: 11873\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1592902577.py:11: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n  answer_lengths = train_data_df['answers'].apply(lambda x: len(x['text'][0]) if x['text'] else 0)\n","output_type":"stream"},{"name":"stdout","text":"\nAnswer length statistics:\ncount    130319.00000\nmean         13.42376\nstd          20.01709\nmin           0.00000\n25%           0.00000\n50%           7.00000\n75%          17.00000\nmax         239.00000\nName: answers, dtype: float64\n\nNumber of unanswerable questions in training data: 43498\nPercentage of unanswerable questions: 33.38%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"2.3 Save a Processed Sample","metadata":{}},{"cell_type":"code","source":"# Cell 3: Save a small sample of the dataset\n# This is useful for rapid prototyping and debugging\nsmall_train_data = train_data.select(range(10000))\nsmall_val_data = val_data.select(range(1000))\n\n# Save the sampled datasets to a local directory\nsmall_train_data.save_to_disk(\"./outputs/small_train_data\")\nsmall_val_data.save_to_disk(\"./outputs/small_val_data\")\n\nprint(\"Saved small training and validation subsets to the 'outputs' directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:20.911960Z","iopub.execute_input":"2025-09-30T15:38:20.912251Z","iopub.status.idle":"2025-09-30T15:38:21.176060Z","shell.execute_reply.started":"2025-09-30T15:38:20.912227Z","shell.execute_reply":"2025-09-30T15:38:21.175482Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d922399f1b14693ae3d3590edf28aee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f775f2e1a9d543bdb406d9f099c0beda"}},"metadata":{}},{"name":"stdout","text":"Saved small training and validation subsets to the 'outputs' directory.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"3. DATA PREPROCESSING FOR BERT","metadata":{}},{"cell_type":"markdown","source":"3.1 Load Tokenizer and Define Preprocessing Function","metadata":{}},{"cell_type":"code","source":"# Cell 1: Load the tokenizer\nimport os\nfrom transformers import AutoTokenizer\n\nmodel_checkpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\nprint(\"Tokenizer loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:21.176822Z","iopub.execute_input":"2025-09-30T15:38:21.177071Z","iopub.status.idle":"2025-09-30T15:38:28.118490Z","shell.execute_reply.started":"2025-09-30T15:38:21.177054Z","shell.execute_reply":"2025-09-30T15:38:28.117857Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"688ce983eecc43d3abeca1f229f6819c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b85e1aefbd25484bbd5b4b26158bada2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f94e0f3968f48e4807c2399f596dc51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6bed59520734270ba05ca37cbb83a3f"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 2: Define the preprocessing function\nfrom datasets import load_dataset\nimport numpy as np\n\n# Load the full dataset (not the sample) for training\nsquad_dataset = load_dataset(\"squad_v2\")\n\nmax_length = 384  # The maximum length of a feature (question and context)\ndoc_stride = 128   # The authorized overlap between two parts of the context\n\ndef prepare_train_features(examples):\n    # Some questions have lots of whitespace, which we remove\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride.\n    # This results in one example possible giving several features when a context is long,\n    # each of those features having a context that overlaps a bit with the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\", # Truncate the context only\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features, we need a map from a feature to its original example.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label all our examples.\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We find the example to which the current feature belongs.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        \n        # If no answers are given, the feature is unanswerable.\n        if len(answers[\"text\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(0)\n            tokenized_examples[\"end_positions\"].append(0)\n        else:\n            # We get the start and end character positions of the answer in the original text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # We find the start and end token indices of the answer.\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            \n            # Find the start and end of the context\n            context_start_token_index = 0\n            while sequence_ids[context_start_token_index] != 1:\n                context_start_token_index += 1\n            \n            context_end_token_index = len(sequence_ids) - 1\n            while sequence_ids[context_end_token_index] != 1:\n                context_end_token_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with 0)\n            if not (offsets[context_start_token_index][0] <= start_char and offsets[context_end_token_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(0)\n                tokenized_examples[\"end_positions\"].append(0)\n            else:\n                # Otherwise, find the start and end token indices of the answer\n                token_start_index = context_start_token_index\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n\n                token_end_index = context_end_token_index\n                while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples\n\nprint(\"Preprocessing function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:28.119192Z","iopub.execute_input":"2025-09-30T15:38:28.119626Z","iopub.status.idle":"2025-09-30T15:38:29.326498Z","shell.execute_reply.started":"2025-09-30T15:38:28.119578Z","shell.execute_reply":"2025-09-30T15:38:29.325867Z"}},"outputs":[{"name":"stdout","text":"Preprocessing function defined.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"3.2 Apply the preprocessing to the Dataset","metadata":{}},{"cell_type":"code","source":"# Cell 3: Apply the function to the dataset\n# This will take some time, especially for the training set\ntokenized_squad = squad_dataset.map(\n    prepare_train_features, \n    batched=True,\n    remove_columns=squad_dataset[\"train\"].column_names\n)\n\nprint(\"\\nDataset tokenization complete!\")\nprint(tokenized_squad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:33.644563Z","iopub.execute_input":"2025-09-30T15:38:33.644886Z","iopub.status.idle":"2025-09-30T15:39:45.217260Z","shell.execute_reply.started":"2025-09-30T15:38:33.644867Z","shell.execute_reply":"2025-09-30T15:39:45.216693Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575eca9f4a1a4c8187e27462ce88a241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df5d6707aff4c0681b4db52d507c863"}},"metadata":{}},{"name":"stdout","text":"\nDataset tokenization complete!\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n        num_rows: 131754\n    })\n    validation: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n        num_rows: 12134\n    })\n})\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"3.3 Save the tokenized Dataset","metadata":{}},{"cell_type":"code","source":"# Cell 4: Save the tokenized dataset to the outputs directory\ntokenized_squad[\"train\"].save_to_disk(\"./outputs/tokenized_squad_train\")\ntokenized_squad[\"validation\"].save_to_disk(\"./outputs/tokenized_squad_validation\")\n\nprint(\"Tokenized dataset saved to ./outputs/.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:39:45.218516Z","iopub.execute_input":"2025-09-30T15:39:45.218832Z","iopub.status.idle":"2025-09-30T15:39:45.652141Z","shell.execute_reply.started":"2025-09-30T15:39:45.218814Z","shell.execute_reply":"2025-09-30T15:39:45.651468Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/131754 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45134836ce44dd590c01c961576673a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/12134 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e130bd7e3a2c4951a225e4c48e04c90f"}},"metadata":{}},{"name":"stdout","text":"Tokenized dataset saved to ./outputs/.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"4. Model Training","metadata":{}},{"cell_type":"markdown","source":"4.1 Load Dataset and Model","metadata":{}},{"cell_type":"code","source":"# Cell 4: Load Tokenized Dataset and Model\nimport os\nimport torch\nfrom datasets import load_from_disk\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\n# Load the tokenized dataset from the 'outputs' directory\ntokenized_squad_train = load_from_disk(\"./outputs/tokenized_squad_train\")\ntokenized_squad_val = load_from_disk(\"./outputs/tokenized_squad_validation\")\n\n# Load the pre-trained BERT model for question answering\nmodel_checkpoint = \"bert-base-uncased\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\nprint(\"Dataset and model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:39:45.652878Z","iopub.execute_input":"2025-09-30T15:39:45.653126Z","iopub.status.idle":"2025-09-30T15:40:06.473674Z","shell.execute_reply.started":"2025-09-30T15:39:45.653110Z","shell.execute_reply":"2025-09-30T15:40:06.473076Z"}},"outputs":[{"name":"stderr","text":"2025-09-30 15:39:51.608844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759246791.804528      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759246791.861315      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aaffad37e0d41dd9b2db5fc7b11f100"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Dataset and model loaded successfully!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"4.2 Define Training Arguments and Trainer","metadata":{}},{"cell_type":"code","source":"# Cell 5: Define Training Arguments and Trainer\nbatch_size = 16\nmodel_name = model_checkpoint.split(\"/\")[-1]\n\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-squad\",\n    eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=2, # Typically 2-3 epochs is sufficient for SQuAD\n    weight_decay=0.01,\n    push_to_hub=False,\n    report_to=\"none\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_squad_train,\n    eval_dataset=tokenized_squad_val,\n    tokenizer=tokenizer, # Important: use the same tokenizer as preprocessing\n)\n\nprint(\"Trainer initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:40:06.474763Z","iopub.execute_input":"2025-09-30T15:40:06.475228Z","iopub.status.idle":"2025-09-30T15:40:06.851520Z","shell.execute_reply.started":"2025-09-30T15:40:06.475210Z","shell.execute_reply":"2025-09-30T15:40:06.850897Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1737058348.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Trainer initialized.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"4.3 Train the Model","metadata":{}},{"cell_type":"code","source":"# Cell 6: Start training\ntrainer.train()\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:40:13.451917Z","iopub.execute_input":"2025-09-30T15:40:13.452964Z","iopub.status.idle":"2025-09-30T18:52:48.410725Z","shell.execute_reply.started":"2025-09-30T15:40:13.452929Z","shell.execute_reply":"2025-09-30T18:52:48.410064Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8236' max='8236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8236/8236 3:12:31, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.118100</td>\n      <td>1.080566</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.859200</td>\n      <td>1.125302</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training complete!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"4.4 Save the Final Model and Predictions","metadata":{}},{"cell_type":"code","source":"final_model_path = \"./outputs/final_model\"\ntrainer.save_model(final_model_path)\nprint(f\"Final model saved to {final_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T18:52:56.258283Z","iopub.execute_input":"2025-09-30T18:52:56.258523Z","iopub.status.idle":"2025-09-30T18:52:57.171635Z","shell.execute_reply.started":"2025-09-30T18:52:56.258507Z","shell.execute_reply":"2025-09-30T18:52:57.170785Z"}},"outputs":[{"name":"stdout","text":"Final model saved to ./outputs/final_model\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"5. Model Evaluation and Inference","metadata":{}},{"cell_type":"markdown","source":"5.1 Run Evaluation","metadata":{}},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T18:54:51.637678Z","iopub.execute_input":"2025-09-30T18:54:51.638219Z","iopub.status.idle":"2025-09-30T18:54:55.435079Z","shell.execute_reply.started":"2025-09-30T18:54:51.638198Z","shell.execute_reply":"2025-09-30T18:54:55.434283Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.6\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Cell 8: Run final evaluation (Corrected to use original dataset for text)\nfrom evaluate import load\nfrom transformers import pipeline\n\n# The metrics are part of the HuggingFace 'evaluate' library\nmetric = load(\"squad_v2\")\n\n# Define the path to your saved model\nfinal_model_path = \"./outputs/final_model\"\n\n# Create a question-answering pipeline with our fine-tuned model\n# Ensure the tokenizer is loaded from the start of the notebook\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=final_model_path,\n    tokenizer=tokenizer,\n    device=0 # Use GPU\n)\n\n# Use the ORIGINAL SQuAD validation set for the question and context text\noriginal_val_data = squad_dataset[\"validation\"]\n\nprint(\"\\nSample Predictions:\")\nnum_samples = 10\nfor i in range(num_samples):\n    question = original_val_data['question'][i]\n    context = original_val_data['context'][i]\n    \n    # Get prediction from the QA pipeline\n    result = qa_pipeline(\n        question=question,\n        context=context\n    )\n    \n    print(f\"Question: {question}\")\n    print(f\"Ground Truth Answer: {original_val_data['answers'][i]['text'] if original_val_data['answers'][i]['text'] else 'Unanswerable'}\")\n    print(f\"Predicted Answer: {result['answer']}\")\n    print(\"-\" * 20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T18:56:13.697858Z","iopub.execute_input":"2025-09-30T18:56:13.698150Z","iopub.status.idle":"2025-09-30T18:56:17.605287Z","shell.execute_reply.started":"2025-09-30T18:56:13.698130Z","shell.execute_reply":"2025-09-30T18:56:17.604712Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\nSample Predictions:\nQuestion: In what country is Normandy located?\nGround Truth Answer: ['France', 'France', 'France', 'France']\nPredicted Answer: France\n--------------------\nQuestion: When were the Normans in Normandy?\nGround Truth Answer: ['10th and 11th centuries', 'in the 10th and 11th centuries', '10th and 11th centuries', '10th and 11th centuries']\nPredicted Answer: 10th and 11th centuries\n--------------------\nQuestion: From which countries did the Norse originate?\nGround Truth Answer: ['Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway']\nPredicted Answer: Denmark, Iceland and Norway\n--------------------\nQuestion: Who was the Norse leader?\nGround Truth Answer: ['Rollo', 'Rollo', 'Rollo', 'Rollo']\nPredicted Answer: Rollo\n--------------------\nQuestion: What century did the Normans first gain their separate identity?\nGround Truth Answer: ['10th century', 'the first half of the 10th century', '10th', '10th']\nPredicted Answer: 10th\n--------------------\nQuestion: Who gave their name to Normandy in the 1000's and 1100's\nGround Truth Answer: Unanswerable\nPredicted Answer: The Normans\n--------------------\nQuestion: What is France a region of?\nGround Truth Answer: Unanswerable\nPredicted Answer: Normandy\n--------------------\nQuestion: Who did King Charles III swear fealty to?\nGround Truth Answer: Unanswerable\nPredicted Answer: West Francia\n--------------------\nQuestion: When did the Frankish identity emerge?\nGround Truth Answer: Unanswerable\nPredicted Answer: first half of the 10th century\n--------------------\nQuestion: Who was the duke in the battle of Hastings?\nGround Truth Answer: ['William the Conqueror', 'William the Conqueror', 'William the Conqueror']\nPredicted Answer: William the Conqueror\n--------------------\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"5.2 Test with a Custom Example","metadata":{}},{"cell_type":"code","source":"# Cell 9: Custom inference\ncustom_context = \"\"\"\nThe Amazon River is the largest river by discharge volume of water in the world, and it is a popular destination for tourists. It is located in South America and flows through Brazil, Peru, and Colombia. The river is home to a wide variety of wildlife, including the Amazon river dolphin and many species of fish.\n\"\"\"\ncustom_question = \"Which continent is the Amazon River in?\"\n\n# Use the pipeline to get the answer\nresult = qa_pipeline(question=custom_question, context=custom_context)\n\nprint(\"Custom Question:\", custom_question)\nprint(\"Context:\", custom_context)\nprint(\"\\nPredicted Answer:\", result['answer'])\nprint(\"Confidence Score:\", round(result['score'], 4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T18:57:00.341153Z","iopub.execute_input":"2025-09-30T18:57:00.341914Z","iopub.status.idle":"2025-09-30T18:57:00.364990Z","shell.execute_reply.started":"2025-09-30T18:57:00.341890Z","shell.execute_reply":"2025-09-30T18:57:00.364265Z"}},"outputs":[{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"Custom Question: Which continent is the Amazon River in?\nContext: \nThe Amazon River is the largest river by discharge volume of water in the world, and it is a popular destination for tourists. It is located in South America and flows through Brazil, Peru, and Colombia. The river is home to a wide variety of wildlife, including the Amazon river dolphin and many species of fish.\n\n\nPredicted Answer: South America\nConfidence Score: 0.6236\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Cell 10 (FINAL CLEANED PUSH): Commit artifacts and push\nimport os\n\n# Ensure we are in the correct directory (optional check)\ntry:\n    os.chdir(\"question-answer-bert-squad\")\nexcept FileNotFoundError:\n    pass\n\n# Stage all changes (This stages the trained model and any other artifact/scripts)\n!git add .\n\n# Commit with a descriptive message\n!git commit -m \"feat: FINAL COMMIT - uploaded trained model and evaluation artifacts.\"\n\n# Push to GitHub\n!git push origin main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:00:32.023844Z","iopub.execute_input":"2025-09-30T19:00:32.024130Z","iopub.status.idle":"2025-09-30T19:01:07.920798Z","shell.execute_reply.started":"2025-09-30T19:00:32.024108Z","shell.execute_reply":"2025-09-30T19:01:07.919722Z"}},"outputs":[{"name":"stdout","text":"On branch main\nYour branch is ahead of 'origin/main' by 1 commit.\n  (use \"git push\" to publish your local commits)\n\nnothing to commit, working tree clean\nEnumerating objects: 26, done.\nCounting objects: 100% (26/26), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (22/22), done.\n^Citing objects:  25% (6/24)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Final command to send your model and artifacts to GitHub\n!git push origin main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:02:12.200360Z","iopub.execute_input":"2025-09-30T19:02:12.200628Z","iopub.status.idle":"2025-09-30T19:03:16.464965Z","shell.execute_reply.started":"2025-09-30T19:02:12.200580Z","shell.execute_reply":"2025-09-30T19:03:16.464023Z"}},"outputs":[{"name":"stdout","text":"Enumerating objects: 26, done.\nCounting objects: 100% (26/26), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (22/22), done.\nWriting objects: 100% (24/24), 1.11 GiB | 23.84 MiB/s, done.\nTotal 24 (delta 3), reused 1 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (3/3), done.\u001b[K\nremote: \u001b[1;31merror\u001b[m: Trace: f0ffd41266500fd338cdfd8899e048c0316ba1fd93b16e9117915167bee3d36d\u001b[K\nremote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\nremote: \u001b[1;31merror\u001b[m: File bert-base-uncased-finetuned-squad/checkpoint-8236/model.safetensors is 415.42 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\nremote: \u001b[1;31merror\u001b[m: File outputs/tokenized_squad_train/data-00000-of-00001.arrow is 293.08 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\nremote: \u001b[1;31merror\u001b[m: File bert-base-uncased-finetuned-squad/checkpoint-8236/optimizer.pt is 830.95 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\nremote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\nTo https://github.com/soumya-grandhi/question-answer-bert-squad.git\n \u001b[31m! [remote rejected]\u001b[m main -> main (pre-receive hook declined)\n\u001b[31merror: failed to push some refs to 'https://github.com/soumya-grandhi/question-answer-bert-squad.git'\n\u001b[m","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Cell 11 (DEFINITIVE FIX): Create .gitignore using standard Python\nimport os\n\ngitignore_content = \"\"\"\n# Ignore HuggingFace training checkpoints\nbert-base-uncased-finetuned-squad/\n# Ignore large tokenized datasets\noutputs/tokenized_squad_train/\noutputs/tokenized_squad_validation/\n# Ignore the final saved model in outputs\noutputs/final_model/\n\"\"\"\n\nwith open(\".gitignore\", \"w\") as f:\n    f.write(gitignore_content.strip())\n\nprint(\"Successfully created .gitignore file.\")\n!cat .gitignore # Verify content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:06:05.338770Z","iopub.execute_input":"2025-09-30T19:06:05.339262Z","iopub.status.idle":"2025-09-30T19:06:05.524335Z","shell.execute_reply.started":"2025-09-30T19:06:05.339239Z","shell.execute_reply":"2025-09-30T19:06:05.523366Z"}},"outputs":[{"name":"stdout","text":"Successfully created .gitignore file.\n# Ignore HuggingFace training checkpoints\nbert-base-uncased-finetuned-squad/\n# Ignore large tokenized datasets\noutputs/tokenized_squad_train/\noutputs/tokenized_squad_validation/\n# Ignore the final saved model in outputs\noutputs/final_model/","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Cell 12: Remove large files from Git history and push\n# This removes the large files from Git's index without deleting them locally\n!git rm -r --cached bert-base-uncased-finetuned-squad/\n!git rm -r --cached outputs/tokenized_squad_train/\n!git rm -r --cached outputs/tokenized_squad_validation/\n!git rm -r --cached outputs/final_model/\n\n# Commit the cleanup and the .gitignore file\n!git add .gitignore\n!git commit -m \"fix: exclude large model and data artifacts from git tracking\"\n\n# Final push (This should resolve the large file error and succeed)\n!git push origin main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:06:26.535408Z","iopub.execute_input":"2025-09-30T19:06:26.536251Z","iopub.status.idle":"2025-09-30T19:07:37.740397Z","shell.execute_reply.started":"2025-09-30T19:06:26.536220Z","shell.execute_reply":"2025-09-30T19:07:37.739409Z"}},"outputs":[{"name":"stdout","text":"rm 'bert-base-uncased-finetuned-squad/checkpoint-8236/config.json'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/model.safetensors'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/optimizer.pt'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/rng_state.pth'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/scheduler.pt'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/special_tokens_map.json'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/tokenizer.json'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/tokenizer_config.json'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/trainer_state.json'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/training_args.bin'\nrm 'bert-base-uncased-finetuned-squad/checkpoint-8236/vocab.txt'\nrm 'outputs/tokenized_squad_train/data-00000-of-00001.arrow'\nrm 'outputs/tokenized_squad_train/dataset_info.json'\nrm 'outputs/tokenized_squad_train/state.json'\nrm 'outputs/tokenized_squad_validation/data-00000-of-00001.arrow'\nrm 'outputs/tokenized_squad_validation/dataset_info.json'\nrm 'outputs/tokenized_squad_validation/state.json'\nrm 'outputs/final_model/config.json'\nrm 'outputs/final_model/model.safetensors'\nrm 'outputs/final_model/special_tokens_map.json'\nrm 'outputs/final_model/tokenizer.json'\nrm 'outputs/final_model/tokenizer_config.json'\nrm 'outputs/final_model/training_args.bin'\nrm 'outputs/final_model/vocab.txt'\n[main 9b100e4] fix: exclude large model and data artifacts from git tracking\n 25 files changed, 7 insertions(+), 122926 deletions(-)\n create mode 100644 .gitignore\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/config.json\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/model.safetensors\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/optimizer.pt\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/rng_state.pth\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/scheduler.pt\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/special_tokens_map.json\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/tokenizer.json\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/tokenizer_config.json\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/trainer_state.json\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/training_args.bin\n delete mode 100644 bert-base-uncased-finetuned-squad/checkpoint-8236/vocab.txt\n delete mode 100644 outputs/final_model/config.json\n delete mode 100644 outputs/final_model/model.safetensors\n delete mode 100644 outputs/final_model/special_tokens_map.json\n delete mode 100644 outputs/final_model/tokenizer.json\n delete mode 100644 outputs/final_model/tokenizer_config.json\n delete mode 100644 outputs/final_model/training_args.bin\n delete mode 100644 outputs/final_model/vocab.txt\n delete mode 100644 outputs/tokenized_squad_train/data-00000-of-00001.arrow\n delete mode 100644 outputs/tokenized_squad_train/dataset_info.json\n delete mode 100644 outputs/tokenized_squad_train/state.json\n delete mode 100644 outputs/tokenized_squad_validation/data-00000-of-00001.arrow\n delete mode 100644 outputs/tokenized_squad_validation/dataset_info.json\n delete mode 100644 outputs/tokenized_squad_validation/state.json\nEnumerating objects: 29, done.\nCounting objects: 100% (29/29), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (25/25), done.\nWriting objects: 100% (27/27), 1.11 GiB | 23.67 MiB/s, done.\nTotal 27 (delta 3), reused 1 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (3/3), done.\u001b[K\nremote: \u001b[1;31merror\u001b[m: Trace: 1e65a628b4826b6165c2aded7eaa3505f37bdbf99400ec6ef9bdbb54ddaf4988\u001b[K\nremote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\nremote: \u001b[1;31merror\u001b[m: File bert-base-uncased-finetuned-squad/checkpoint-8236/model.safetensors is 415.42 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\nremote: \u001b[1;31merror\u001b[m: File outputs/tokenized_squad_train/data-00000-of-00001.arrow is 293.08 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\nremote: \u001b[1;31merror\u001b[m: File bert-base-uncased-finetuned-squad/checkpoint-8236/optimizer.pt is 830.95 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\nremote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\nTo https://github.com/soumya-grandhi/question-answer-bert-squad.git\n \u001b[31m! [remote rejected]\u001b[m main -> main (pre-receive hook declined)\n\u001b[31merror: failed to push some refs to 'https://github.com/soumya-grandhi/question-answer-bert-squad.git'\n\u001b[m","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Cell 13: Squash last two commits into one clean commit\n# This rewrites history to remove the large files completely\n!git reset --soft HEAD~2\n!git commit -m \"feat: complete project code and documentation (EXCLUDING large artifacts)\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:09:47.241576Z","iopub.execute_input":"2025-09-30T19:09:47.241934Z","iopub.status.idle":"2025-09-30T19:09:47.602461Z","shell.execute_reply.started":"2025-09-30T19:09:47.241907Z","shell.execute_reply":"2025-09-30T19:09:47.601725Z"}},"outputs":[{"name":"stdout","text":"[main d7bc50a] feat: complete project code and documentation (EXCLUDING large artifacts)\n 1 file changed, 7 insertions(+)\n create mode 100644 .gitignore\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Cell 14: Force push to overwrite remote history with the clean commit\n!git push origin main --force","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T19:09:56.305528Z","iopub.execute_input":"2025-09-30T19:09:56.306243Z","iopub.status.idle":"2025-09-30T19:09:56.980923Z","shell.execute_reply.started":"2025-09-30T19:09:56.306215Z","shell.execute_reply":"2025-09-30T19:09:56.980060Z"}},"outputs":[{"name":"stdout","text":"Enumerating objects: 4, done.\nCounting objects: 100% (4/4), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 513 bytes | 513.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nTo https://github.com/soumya-grandhi/question-answer-bert-squad.git\n   8a19a0b..d7bc50a  main -> main\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}