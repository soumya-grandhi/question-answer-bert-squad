# Question Answering with BERT and SQuAD v2

## Project Objective
Train and evaluate a BERT-based model on the Stanford Question Answering Dataset (SQuAD) v2 to answer questions given a context passage, including identifying unanswerable questions.

## Workflow and Environment
This project was developed using a **Kaggle-First workflow**, with all training and experimentation performed on a Kaggle GPU Notebook.

- **Primary Framework:** PyTorch & HuggingFace Transformers
- **Training Environment:** Kaggle Notebook (GPU-enabled)
- **Code:** This repository contains the code and configuration files.
- **Trained Model:** The trained model is excluded from this repository due to GitHub's 100MB limit. The model can be accessed via the **Kaggle Outputs Dataset** (link below).

## Project Structure
- `notebooks/`: Contains the final, runnable notebook (`qa-bert-squad-complete.ipynb`).
- `outputs/`: Tracks the artifacts directory, but the large files are ignored via `.gitignore`.
- `requirements.txt`: Lists all Python dependencies.

## Model Artifacts (The Trained Model)
The fine-tuned BERT model (saved in `outputs/final_model`) can be found here:
[Insert Kaggle Dataset Link Here after publishing]
